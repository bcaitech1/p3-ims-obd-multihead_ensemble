{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EfficientDet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "73d1034e5a7d4adab1853f1b7c83aa18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dcab505c497745b6b894000d271a72fb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4a612ce702c34e7c95e0e248f63bc87d",
              "IPY_MODEL_4b98af69510c4b678bcac6e224e7fc83"
            ]
          }
        },
        "dcab505c497745b6b894000d271a72fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a612ce702c34e7c95e0e248f63bc87d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_adf7b4a5db284a7eb224805dac8b464a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 837,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 837,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73d2ced1be9a437daf1074b1e9c792d5"
          }
        },
        "4b98af69510c4b678bcac6e224e7fc83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2627eccbcefe4adf80d68a682157d487",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 837/837 [01:18&lt;00:00, 10.70it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b48a468c985a45caa7a187249eaa120f"
          }
        },
        "adf7b4a5db284a7eb224805dac8b464a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73d2ced1be9a437daf1074b1e9c792d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2627eccbcefe4adf80d68a682157d487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b48a468c985a45caa7a187249eaa120f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "51xD3nsjJSd9"
      },
      "source": [
        "!pip install albumentations==0.5.2\n",
        "!pip install efficientnet_pytorch\n",
        "!pip install pycocotools\n",
        "!pip install cython\n",
        "!pip install timm\n",
        "!pip install effdet\n",
        "!pip install gluoncv\n",
        "!pip install mxnet\n",
        "!pip install wandb\n",
        "!pip install madgrad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d23ELzXVTZ6q"
      },
      "source": [
        "# !wget https://prod-aistages-public.s3.amazonaws.com/app/Competitions/000035/data/data.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T_fBKxmT8yl"
      },
      "source": [
        "# import os\n",
        "# os.chdir('/content/input/data')\n",
        "# !tar -xzf /content/data.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dz_t7TKKLsx",
        "outputId": "12854f42-89d5-4096-d880-27bca60f5df0"
      },
      "source": [
        "from pycocotools.coco import COCO\n",
        "import gluoncv.utils as gcv\n",
        "import torch\n",
        "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain , DetBenchPredict\n",
        "from effdet.efficientdet import HeadNet\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "import wandb\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import math\n",
        "from scheduler import *\n",
        "from madgrad import MADGRAD\n",
        "warnings.filterwarnings(action='ignore') \n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUXVqeCIKOfk"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcDnSaTpKPgm"
      },
      "source": [
        "batch_size = 6\n",
        "learning_rate = 1e-3\n",
        "max_lr = learning_rate\n",
        "min_lr = 1e-6\n",
        "num_epochs = 100\n",
        "seed = 2021\n",
        "seed_everything(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl5SL4fLKPeL"
      },
      "source": [
        "IMAGENET_DEFAULT_MEAN = [x * 255 for x in (0.485, 0.456, 0.406)]\n",
        "IMAGENET_DEFAULT_STD = [x * 255 for x in (0.229, 0.224, 0.225)]\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    '''\n",
        "      data_dir: data가 존재하는 폴더 경로\n",
        "      transforms: data transform (resize, crop, Totensor, etc,,,)\n",
        "    '''\n",
        "    def __init__(self, annotation, data_dir, transforms , type = 'train'):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        # coco annotation 불러오기 (coco API)\n",
        "        self.coco = COCO(annotation)\n",
        "        self.predictions = {\n",
        "            \"images\": self.coco.dataset[\"images\"].copy(),\n",
        "            \"categories\": self.coco.dataset[\"categories\"].copy(),\n",
        "            \"annotations\": None\n",
        "        }\n",
        "        self.transforms = transforms\n",
        "        self.type = type\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        image_id = self.coco.getImgIds(imgIds=index)\n",
        "        image, boxes, labels = self.load_image_and_boxes(index)\n",
        "\n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels\n",
        "        target['image_id'] = torch.tensor([index])\n",
        "        \n",
        "        # Multi Scale\n",
        "        target['img_size'] = torch.tensor([(512, 512)])\n",
        "        target['img_scale'] = torch.tensor([1.])\n",
        "        \n",
        "        if self.transforms:\n",
        "            # Transform 적용 후 Box가 없다면 있을 때 까지 반복 (default = 10)\n",
        "            for i in range(10):\n",
        "                sample = {\n",
        "                    'image': image,\n",
        "                    'bboxes': target['boxes'],\n",
        "                    'labels': target['labels']\n",
        "                }\n",
        "                sample = self.transforms(**sample)\n",
        "                if len(sample['bboxes']) > 0:\n",
        "                    image = sample['image']\n",
        "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n",
        "                    target['labels'] = torch.tensor(sample['labels'])\n",
        "                    break\n",
        "        \n",
        "        return image, target, image_id\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.coco.getImgIds())\n",
        "    \n",
        "    def load_image_and_boxes(self, index):\n",
        "        image_id = self.coco.getImgIds(imgIds=index)\n",
        "        image_info = self.coco.loadImgs(image_id)[0]\n",
        "        \n",
        "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image = (image - IMAGENET_DEFAULT_MEAN) / IMAGENET_DEFAULT_STD\n",
        "\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        boxes = np.array([x['bbox'] for x in anns])\n",
        "        \n",
        "        labels = np.array([x['category_id'] for x in anns])\n",
        "        #labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # boxex (x_min, y_min, x_max, y_max)\n",
        "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
        "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
        "        return image, boxes, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRVuKwhbKPcI",
        "outputId": "4beada3c-5909-410c-90cf-2b9b335f0a98"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "def get_train_transform():\n",
        "    return A.Compose([\n",
        "        A.Resize(512, 512),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params=A.BboxParams(\n",
        "            format='pascal_voc',\n",
        "            min_area=0, \n",
        "            min_visibility=0,\n",
        "            label_fields=['labels'])\n",
        "    )\n",
        "\n",
        "def get_valid_transform():\n",
        "    return A.Compose([\n",
        "        A.Resize(512, 512),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params=A.BboxParams(\n",
        "            format='pascal_voc',\n",
        "            min_area=0, \n",
        "            min_visibility=0,\n",
        "            label_fields=['labels'])\n",
        "    )\n",
        "\n",
        "train_annotation = '/content/input/data/train.json'\n",
        "val_annotation = '/content/input/data/val.json'\n",
        "data_dir = '/content/input/data'\n",
        "train_dataset = CustomDataset(train_annotation, data_dir, get_train_transform())\n",
        "val_dataset = CustomDataset(val_annotation, data_dir, get_valid_transform())\n",
        "\n",
        "train_data_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True,collate_fn=collate_fn,num_workers=2)\n",
        "val_data_loader=DataLoader(val_dataset,batch_size=batch_size,shuffle=False,collate_fn=collate_fn,num_workers=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=3.84s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.87s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNfbOdiSMNDK"
      },
      "source": [
        "def get_model():\n",
        "    # get weight\n",
        "    # url : https://github.com/rwightman/efficientdet-pytorch\n",
        "    config = get_efficientdet_config('tf_efficientdet_d4')\n",
        "    config.image_size = (512, 512)\n",
        "    config.norm_kwargs=dict(eps=.001, momentum=.01)\n",
        "    net = EfficientDet(config, pretrained_backbone=True)\n",
        "    checkpoint = torch.load('/content/drive/MyDrive/efficientdet_d4-5b370b7a.pth')\n",
        "    net.load_state_dict(checkpoint)\n",
        "\n",
        "    net.reset_head(num_classes=11)\n",
        "    net.class_net = HeadNet(config, num_outputs=11)\n",
        "    return DetBenchTrain(net, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5fDLw5TMYod"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=50, verbose=False, delta=0, path=None):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_mAP, model):\n",
        "        score = val_mAP\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        model.eval()\n",
        "        torch.save(model.state_dict(), self.path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwFP-bnmZlp8"
      },
      "source": [
        "def get_learning_rate(optimizer):\n",
        "    lr = []\n",
        "    for param_group in optimizer.param_groups:\n",
        "        lr += [param_group['lr']]\n",
        "    return lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoM4TRZIc4iO"
      },
      "source": [
        "def train_fn(model, train_data_loader , val_data_loader, optimizer,scheduler, device):\n",
        "    model.train()\n",
        "    train_loss = AverageMeter()\n",
        "    train_cls_loss = AverageMeter()\n",
        "    train_box_loss = AverageMeter()\n",
        "\n",
        "    save_path = '/content/drive/MyDrive/saved/checkpoint.pth'\n",
        "    early_stop = EarlyStopping(path=save_path)\n",
        "    for epoch in range(num_epochs):\n",
        "        ################################# train ###############################\n",
        "        train_loss.reset()\n",
        "        train_cls_loss.reset()\n",
        "        train_box_loss.reset()\n",
        "        model.train()\n",
        "        for images, targets, image_ids in tqdm(train_data_loader):\n",
        "            # gpu 계산을 위해 image.to(device)\n",
        "            images = torch.stack(images)\n",
        "            images = images.to(device).float()\n",
        "            current_batch_size = images.shape[0]\n",
        "\n",
        "            targets2 = {}\n",
        "            targets2['bbox'] = [target['boxes'].to(device).float() for target in targets] # variable number of instances, so the entire structure can be forced to tensor\n",
        "            targets2['cls'] = [target['labels'].to(device).float() for target in targets]\n",
        "            targets2['image_id'] = torch.tensor([target['image_id'] for target in targets]).to(device).float()\n",
        "            targets2['img_scale'] = torch.tensor([target['img_scale'] for target in targets]).to(device).float()\n",
        "            targets2['img_size'] = torch.tensor([(512, 512) for target in targets]).to(device).float()\n",
        "\n",
        "            print(images)\n",
        "            # calculate loss\n",
        "            losses, cls_loss, box_loss = model(images, targets2).values()\n",
        "\n",
        "            train_loss.update(losses.detach().item(), current_batch_size)\n",
        "            train_cls_loss.update(cls_loss.detach().item(), current_batch_size)\n",
        "            train_box_loss.update(box_loss.detach().item(), current_batch_size)\n",
        "\n",
        "            # backward\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
        "            optimizer.step()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "            wandb.log({'Learning rate': get_learning_rate(optimizer)[0]})       \n",
        "\n",
        "        # Evaluation\n",
        "        val_loss, val_mAP = vali_fn(val_data_loader,model,device)\n",
        "        \n",
        "        print(f\"\\nEpoch #{epoch+1} train loss: [{train_loss.avg}] train cls loss : [{train_cls_loss.avg}] train box loss : [{train_box_loss.avg}] validation mAP : [{val_mAP}] \\n\")\n",
        "        wandb.log({\"Train Loss\":train_loss.avg, \"Validation mAP@50\":val_mAP})\n",
        "        early_stop(val_mAP,model)\n",
        "        if early_stop.early_stop:\n",
        "            print('Stop Training.....')\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ircq9zzwMgGb"
      },
      "source": [
        "def vali_fn(val_data_loader, model, device):\n",
        "    model.eval()\n",
        "    vali_loss = AverageMeter()\n",
        "    vali_mAP = AverageMeter()\n",
        "    # Custom\n",
        "    metric = gcv.metrics.VOCMApMetric(iou_thresh=0.5)\n",
        "    with torch.no_grad():\n",
        "        for images, targets, image_ids in tqdm(val_data_loader):\n",
        "            # gpu 계산을 위해 image.to(device)\n",
        "            images = torch.stack(images)\n",
        "            images = images.to(device).float()\n",
        "\n",
        "            current_batch_size = images.shape[0]\n",
        "\n",
        "            targets2 = {}\n",
        "            targets2['bbox'] = [target['boxes'].to(device).float() for target in targets] # variable number of instances, so the entire structure can be forced to tensor\n",
        "            targets2['cls'] = [target['labels'].to(device).float() for target in targets]\n",
        "            targets2['image_id'] = torch.tensor([target['image_id'] for target in targets]).to(device).float()\n",
        "            targets2['img_scale'] = torch.tensor([target['img_scale'] for target in targets]).to(device).float()\n",
        "            targets2['img_size'] = torch.tensor([(512, 512) for target in targets]).to(device).float()\n",
        "\n",
        "            outputs = model(images, targets2)\n",
        "\n",
        "            loss = outputs['loss']\n",
        "            det = outputs['detections']\n",
        "\n",
        "            # Calc Metric\n",
        "            for i in range(0, len(det)):\n",
        "                pred_scores=det[i, :, 4].cpu().unsqueeze_(0).numpy()\n",
        "                condition=(pred_scores > 0.05)[0]\n",
        "                gt_boxes=targets2['bbox'][i].cpu().unsqueeze_(0).numpy()[...,[1,0,3,2]] #move to PASCAL VOC from yxyx format\n",
        "                gt_labels=targets2['cls'][i].cpu().unsqueeze_(0).numpy()\n",
        "\n",
        "                pred_bboxes=det[i, :, 0:4].cpu().unsqueeze_(0).numpy()[:, condition, :]\n",
        "                pred_labels=det[i, :, 5].cpu().unsqueeze_(0).numpy()[:, condition]\n",
        "                pred_scores=pred_scores[:, condition]\n",
        "                metric.update(\n",
        "                  pred_bboxes=pred_bboxes,\n",
        "                  pred_labels=pred_labels,\n",
        "                  pred_scores=pred_scores,\n",
        "                  gt_bboxes=gt_boxes,\n",
        "                  gt_labels=gt_labels)\n",
        "\n",
        "            vali_mAP.update(metric.get()[1], current_batch_size)\n",
        "            vali_loss.update(loss.detach().item(), current_batch_size)\n",
        "    \n",
        "    # validation loss\n",
        "    return vali_loss.avg, vali_mAP.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ8PHMkvMheZ"
      },
      "source": [
        "# Model\n",
        "model = get_model()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/saved/checkpoint.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "# setting\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=learning_rate, momentum = 0.9)\n",
        "scheduler = None\n",
        "# first_cycle_steps = len(train_data_loader)* num_epochs // 2\n",
        "# scheduler = CosineAnnealingWarmupRestarts(\n",
        "#     optimizer, \n",
        "#     first_cycle_steps=first_cycle_steps, \n",
        "#     cycle_mult=1.0, \n",
        "#     max_lr=max_lr, \n",
        "#     min_lr=min_lr, \n",
        "#     warmup_steps=int(first_cycle_steps * 0.25), \n",
        "#     gamma=0.5\n",
        "# )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IMnJVxaNcHW"
      },
      "source": [
        "wandb.login()\n",
        "wandb.init(project='P-stage3-detection', name=\"EfficientDet\")\n",
        "train_fn(model, train_data_loader , val_data_loader, optimizer, scheduler , device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIwgrOyhGyW7"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmkoHfNDG4f9"
      },
      "source": [
        "class TestDataset(Dataset):\n",
        "    '''\n",
        "      data_dir: data가 존재하는 폴더 경로\n",
        "      transforms: data transform (resize, crop, Totensor, etc,,,)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, annotation, data_dir, transforms=None):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        # coco annotation 불러오기 (coco API)\n",
        "        self.coco = COCO(annotation)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        \n",
        "        image_id = self.coco.getImgIds(imgIds=index)\n",
        "\n",
        "        image_info = self.coco.loadImgs(image_id)[0]\n",
        "        \n",
        "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        # transform\n",
        "        if self.transforms:\n",
        "            sample = {'image': image}\n",
        "            sample = self.transforms(**sample)\n",
        "            image = sample['image']\n",
        "\n",
        "        return image , image_id\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.coco.getImgIds())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-mlcj2QI_F_"
      },
      "source": [
        "def get_model():\n",
        "    # get weight\n",
        "    # url : https://github.com/rwightman/efficientdet-pytorch\n",
        "    config = get_efficientdet_config('tf_efficientdet_d4')\n",
        "    config.image_size = (512, 512)\n",
        "    config.norm_kwargs=dict(eps=.001, momentum=.01)\n",
        "    net = EfficientDet(config, pretrained_backbone=True)\n",
        "\n",
        "    net.reset_head(num_classes=11)\n",
        "    net.class_net = HeadNet(config, num_outputs=11)\n",
        "    return DetBenchPredict(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1oueOxeOqhA"
      },
      "source": [
        "def predict(test_data_loader, model, device):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    with torch.no_grad():\n",
        "        for images, image_ids in tqdm(test_data_loader):\n",
        "            # gpu 계산을 위해 image.to(device)\n",
        "            images = torch.stack(images)\n",
        "            images = images.to(device).float()\n",
        "\n",
        "            current_batch_size = images.shape[0]\n",
        "\n",
        "            det = model(images)\n",
        "              \n",
        "            # Calc Metric\n",
        "            # 좌표, cs , label\n",
        "            for i in range(0, len(det)):\n",
        "                pred_scores=det[i, :, 4].cpu().unsqueeze_(0).numpy()\n",
        "                condition=(pred_scores > 0.00)[0]\n",
        "                pred_bboxes=det[i, :, 0:4].cpu().unsqueeze_(0).numpy()[:, condition, :]\n",
        "                pred_labels=det[i, :, 5].cpu().unsqueeze_(0).numpy()[:, condition]\n",
        "                pred_scores=pred_scores[:, condition]\n",
        "                outputs.append({'boxes': pred_bboxes.tolist(), 'scores': pred_scores.tolist(), 'labels': pred_labels.tolist()})\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeBbVhCFd2oG"
      },
      "source": [
        "def infernece(model , test_data_loader, device):\n",
        "    model.eval()\n",
        "    outputs = predict(test_data_loader, model, device)\n",
        "    prediction_strings = []\n",
        "    file_names = []\n",
        "    coco = COCO(test_annotation)\n",
        "    score_threshold = 0.00\n",
        "    for i, output in enumerate(outputs):   \n",
        "        prediction_string = ''\n",
        "        image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
        "        for box, score, label in zip(output['boxes'][0], output['scores'][0], output['labels'][0]): \n",
        "            if score > score_threshold and  all(0<= elem <=512 for elem in box) :\n",
        "                  prediction_string += str(int(label)) + ' ' + str(score) + ' ' + str(box[0]) + ' ' + str(\n",
        "                      box[1]) + ' ' + str(box[2]) + ' ' + str(box[3]) + ' '\n",
        "\n",
        "        prediction_strings.append(prediction_string)\n",
        "        file_names.append(image_info['file_name'])\n",
        "    submission = pd.DataFrame()\n",
        "    submission['PredictionString'] = prediction_strings\n",
        "    submission['image_id'] = file_names\n",
        "    submission.to_csv(f'Efficientdet_submission.csv', index=None)\n",
        "    print(submission.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326,
          "referenced_widgets": [
            "73d1034e5a7d4adab1853f1b7c83aa18",
            "dcab505c497745b6b894000d271a72fb",
            "4a612ce702c34e7c95e0e248f63bc87d",
            "4b98af69510c4b678bcac6e224e7fc83",
            "adf7b4a5db284a7eb224805dac8b464a",
            "73d2ced1be9a437daf1074b1e9c792d5",
            "2627eccbcefe4adf80d68a682157d487",
            "b48a468c985a45caa7a187249eaa120f"
          ]
        },
        "id": "E3zbqY6i5IlQ",
        "outputId": "37db88fb-6596-4c1b-8bb3-47ce83a3378c"
      },
      "source": [
        "def get_test_transform():\n",
        "    return A.Compose([\n",
        "        A.Resize(512, 512 , p =1.0),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ])\n",
        "\n",
        "test_annotation = '/content/input/data/test.json'\n",
        "test_data_dir = '/content/input/data'\n",
        "test_dataset = TestDataset(test_annotation, test_data_dir, get_test_transform())\n",
        "\n",
        "test_data_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "#Model\n",
        "model = get_model()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/saved/checkpoint.pth'))\n",
        "model.to(device)\n",
        "\n",
        "# inference\n",
        "infernece(model, test_data_loader, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73d1034e5a7d4adab1853f1b7c83aa18",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=837.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "                                    PredictionString              image_id\n",
            "0  8 0.9980921149253845 173.08245849609375 236.41...  batch_01_vt/0021.jpg\n",
            "1  8 0.557726263999939 285.8876647949219 250.9930...  batch_01_vt/0028.jpg\n",
            "2  1 0.9940304160118103 75.79487609863281 169.357...  batch_01_vt/0031.jpg\n",
            "3  8 0.070650115609169 296.2857971191406 266.9136...  batch_01_vt/0032.jpg\n",
            "4  7 0.9985429048538208 0.5882797241210938 86.769...  batch_01_vt/0070.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "Mh58GIjXBfXE",
        "outputId": "c3a064ba-66c3-4615-bbb2-f7bd902a458f"
      },
      "source": [
        "df = pd.read_csv('/content/input/data/Efficientdet_submission.csv')\n",
        "df['PredictionString'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'8 0.9980921149253845 173.08245849609375 236.4196014404297 467.82977294921875 493.4827880859375 7 0.09756028652191162 133.5152587890625 49.27411651611328 257.794189453125 171.14093017578125 3 0.09317931532859802 134.34291076660156 47.38205337524414 256.5142822265625 166.55084228515625 7 0.05943967401981354 139.29913330078125 47.24940490722656 341.6982421875 231.56031799316406 2 0.05539894104003906 133.24815368652344 48.06289291381836 257.19000244140625 169.51516723632812 3 0.053076907992362976 135.30203247070312 51.81950378417969 334.4064025878906 233.2051544189453 7 0.048682838678359985 174.8128662109375 231.37738037109375 465.49237060546875 488.66650390625 6 0.04674661532044411 424.21710205078125 198.18186950683594 511.4564208984375 231.6643829345703 1 0.04479493573307991 133.5152587890625 49.27411651611328 257.794189453125 171.14093017578125 2 0.039826467633247375 178.88735961914062 236.97039794921875 474.1406555175781 490.8555908203125 6 0.03944314271211624 178.88735961914062 236.97039794921875 474.1406555175781 490.8555908203125 1 0.0334743857383728 178.88735961914062 236.97039794921875 474.1406555175781 490.8555908203125 7 0.03275987133383751 320.20574951171875 41.66455078125 367.0821533203125 156.68380737304688 9 0.030689040198922157 23.596031188964844 123.11602020263672 184.35842895507812 195.13882446289062 5 0.028848694637417793 173.10897827148438 236.30804443359375 469.6916809082031 493.05096435546875 9 0.027007240802049637 15.189735412597656 71.04437255859375 151.46633911132812 154.71221923828125 1 0.025715192779898643 135.30203247070312 51.81950378417969 334.4064025878906 233.2051544189453 10 0.024993985891342163 170.05201721191406 232.33172607421875 468.99761962890625 492.505859375 1 0.02446180023252964 12.098381042480469 5.945674896240234 204.78524780273438 85.42330932617188 2 0.024294374510645866 10.552230834960938 144.5395965576172 151.1499786376953 197.12037658691406 7 0.023135710507631302 164.44915771484375 20.935089111328125 375.292236328125 309.2501525878906 4 0.02175925113260746 3.2204666137695312 55.74302291870117 135.890380859375 144.61056518554688 8 0.020742589607834816 7.65032958984375 206.96783447265625 430.79840087890625 508.51568603515625 7 0.020618103444576263 117.35922241210938 225.17105102539062 190.60968017578125 257.8443603515625 3 0.018599167466163635 34.10728454589844 49.09479522705078 261.92535400390625 179.27899169921875 7 0.017650233581662178 13.239959716796875 201.2606201171875 272.8879089355469 278.2900390625 3 0.017517942935228348 10.135177612304688 55.04607391357422 123.34548950195312 162.45779418945312 9 0.017279718071222305 319.9991760253906 40.22847366333008 367.8097839355469 156.5726776123047 8 0.01693064533174038 209.7921600341797 36.73818588256836 252.3324737548828 72.29200744628906 2 0.016895664855837822 473.1488342285156 271.4587707519531 501.7567443847656 300.9589538574219 3 0.016523512080311775 150.87359619140625 4.237483978271484 278.12518310546875 96.78524780273438 2 0.016279246658086777 10.176345825195312 112.39952087402344 161.23947143554688 161.8332061767578 1 0.01588253118097782 466.224609375 273.5755310058594 494.9595947265625 294.7298889160156 6 0.01573857292532921 155.14881896972656 4.345532417297363 241.1343231201172 26.282150268554688 9 0.015698255971074104 17.385372161865234 88.93122100830078 121.85981750488281 213.94528198242188 6 0.015660811215639114 166.85079956054688 70.71723937988281 255.59414672851562 167.38453674316406 9 0.015205584466457367 18.74658203125 165.53118896484375 159.50938415527344 203.88946533203125 9 0.015107424929738045 21.385330200195312 112.9819564819336 166.56251525878906 161.8909912109375 7 0.014898124150931835 12.36566162109375 249.4137725830078 196.27249145507812 318.65655517578125 9 0.014795294031500816 24.94390106201172 152.6458282470703 158.12051391601562 188.07557678222656 6 0.014553670771420002 12.096000671386719 332.6720275878906 122.8029556274414 447.4455261230469 7 0.01448637992143631 164.73130798339844 1.6695060729980469 377.2117919921875 94.42861938476562 1 0.013871172443032265 84.67882537841797 10.030380249023438 187.93759155273438 83.6473159790039 3 0.013734393753111362 384.29876708984375 34.111507415771484 397.214111328125 160.16566467285156 2 0.01370274554938078 466.224609375 273.5755310058594 494.9595947265625 294.7298889160156 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    }
  ]
}